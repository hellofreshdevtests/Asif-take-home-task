**HelloFresh Data Engineering Test - Solution (Asif Akram)**<br><br>
The task required developing an ETL process that takes input from .json files, processes the data using Apache Spark framework and sends an output csv file of the processed data. 
Firstly, the test repository was forked onto a personal repository and a branch was established. A temporary personal access token was created to establish a connection to DataBricks to process the data using the capabilities of the Spark framework provided in Databricks. A general flow of the process is provided in the chart below: 
![Alt text](functions/Flowchart.png?raw=true)
**Databricks:** Databricks was chosen as the primary host of the processing framework because of its capability of cloning github repositories and providing version control from there. Additionally, the existing framwork of Python and Spark within Databricks negates the need for creating new dependencies and a seperate environment for deploying the application. Databricks also scales storage for future CI/CD implementations, as well as computing resources depending on demand. <br>
<br>**Process:** After Github -> Databricks repository was established, the input json files were read by fetching the files from the input folder in Github using the requests module in Python. Upon inspection the each json file contained individual json objects which were split by line and parsed onto a pandas Dataframe. Data quality checks and unit tests were included in the script and supporting functions to ensure the data being processed is in the correct format, is not empty, contains expected length of rows & columns and acts as expected. The Dataframe was then converted to a Spark schema for processing. Recipes that contained beef in their ingredients list were extracted into a seperate dataframe and further conversions were done such as converting the ISO time format for cookTime & prepTime to seconds using extraction of regex expressions in new columns and calculating the total time taken. The extracted dataframe was then grouped by the difficulty level and average of the total time taken among them. The dataframe was then converted to a pandas dataframe and then converted to csv using the dbutils module. The DataBricks File Storage was used to store the csv in the storage system and the script allows for downloading the csv and uploading into the forked Git repository. This step is a placeholder for the test, other considerations for building a fully automated ETL pipeline can use DataBricks scheduling using dedicated clusters and storing in cloud databases such as S3.
<br><br>**Application run steps:** The application can be run on Databricks using the [[Script module](https://hf-anz-ds.cloud.databricks.com/?o=2322873405429936#notebook/2734653228325816/command/2734653228328612)], some libraries and modules required are as follows: (os, pandas, pyspark, json, requests and csv). The data quality checks and unit test functions subsequently run within the main Script. The last command line acquires a url download link to download the output csv which can then be uploaded into the output folder in the Github repository. 
<br><br>**Consideration for ETL scalability and implementation of CI/CD:** The current ETL build is dedicated to be run by a user upon their discretion. This is mainly due to the step of downloading the output csv file and upload to the Github repo. This can be automated for a CI/CD pipeline by mounting the github repo to DataBricks file system using dbutils module and reading and writing the input-output files to run on a schedule in Databricks to then send pull request to the Github repo. Alternatively, Github action workflows can also be used to automate jobs with the addition of configuration files and modules/libraries in requirements file. For scalability, DataBricks is a good option as it can handle processing large scale of data horizontally across clusters as needed. It also allows for version control much like Git. In case of diagnosis and tuning of the application, further unit tests and quality checks can be built on Databricks on top of the existing ones in the current workflow.
